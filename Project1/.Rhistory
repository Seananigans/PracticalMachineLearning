library(ggplot2)
##############################################################
#Import Chemistry Data
setwd("data")
original<-read.xlsx("HH_Titanium.xlsx", sheetIndex=1)
setwd(HH_Ti_Dir)
##############################################################
View(original)
test <- original
HH <- original
original <- original[original$FIN_TI!=0,]
original <- original[original$DG_TI!=150,]
hist(original$TAP_TI)
hist(original$DG_TI)
hist(original$Q20TI-original$Q11TI)
plot(original$TAP_TI,original$Q20TI-original$Q10TI)
plot(original$TAP_TI,original$Q20TI-original$Q11TI)
cor(original$TAP_TI,original$Q20TI-original$Q10TI)
cor(original$TAP_TI,original$Q20TI-original$Q11TI)
plot(original$TAP_TI+original$DG_TI,original$FIN_TI-original$Q11TI)
cor(original$TAP_TI+original$DG_TI,original$FIN_TI-original$Q11TI)
lm.fit <- lm(FIN_TI~I(TAP_TI+DG_TI),data=original)
summary(lm.fit)
lm.full <- lm(FIN_TI~.-NO_HT_LONG-Q20TI,data=original)
summary(lm.full)
totalAdditions <- original$TAP_TI+original$DG_TI
totalTiChange <- original$FIN_TI-original$Q10TI
plot(totalAdditions,totalTiChange)
plot(totalAdditions,totalTiChange,
main = "HH Ti Change \nQ10 to Final",
xlab=paste("Additions \nCorrelation = ",round(cor(totalAdditions,totalTiChange),4)
),
ylab="Ti Change"
)
View(HH)
HHrecovery <- mean(totalTiChange)/mean(totalAdditions)
HHrecovery
totalAdditions <- HH$TAP_TI+HH$DG_TI
totalTiChange <- original$FIN_TI-original$Q10TI
HHrecovery <- mean(totalTiChange)/mean(totalAdditions)
HHrecovery
totalAdditions <- HH$TAP_TI+HH$DG_TI
totalTiChange <- HH$FIN_TI-HH$Q10TI
HHrecovery <- mean(totalTiChange)/mean(totalAdditions)
HHrecovery
AHH <- original
AHHtotalAdditions <- AHH$TAP_TI+AHH$DG_TI
AHHtotalTiChange <- AHH$FIN_TI-AHH$Q10TI
plot(AHHtotalAdditions,AHHtotalTiChange,
main = "AHH Ti Change \nQ10 to Final",
xlab=paste("Additions \nCorrelation = ",round(cor(AHHtotalAdditions,AHHtotalTiChange),4)
),
ylab="Ti Change"
)
cor(totalTiChange,totalAdditions)
AHHrecovery <- mean(AHHtotalTiChange)/mean(AHHtotalAdditions)
AHHrecovery
#Import Chemistry Data
setwd("data")
original<-read.xlsx("AHH_Titanium.xlsx", sheetIndex=1)
setwd(AHH_Ti_Dir)
AHH <- original
original <- original[original$FIN_TI!=0,]
original <- original[original$DG_TI!=150,]
AHH <- original
AHHtotalAdditions <- AHH$TAP_TI+AHH$DG_TI
AHHtotalTiChange <- AHH$FIN_TI-AHH$Q10TI
plot(AHHtotalAdditions,AHHtotalTiChange,
main = "AHH Ti Change \nQ10 to Final",
xlab=paste("Additions \nCorrelation = ",round(cor(AHHtotalAdditions,AHHtotalTiChange),4)
),
ylab="Ti Change"
)
cor(totalTiChange,totalAdditions)
AHHrecovery <- mean(AHHtotalTiChange)/mean(AHHtotalAdditions)
AHHrecovery
1-AHHrecovery/HHrecovery
setwd("C:/Users/shegarty/Desktop/AHH")
###############################
#Project 1 Directory Setup
AHH_Ti_Dir <- getwd()
if (AHH_Ti_Dir!="C:/Users/shegarty/Desktop/AHH"){
print("Changing Directory")
setwd("C:/Users/shegarty/Desktop/AHH")
AHH_Ti_Dir<- getwd()
}
#Set up directories for figures and new Data
if (!file.exists("figures")) {
print("Creating figures directory")
dir.create("figures")
}
if (!file.exists("data")) {
print("Creating data directory")
dir.create("data")
}
##############################################################
#Import libraries
library(xlsx)
library(dplyr)
library(ggplot2)
#Import Chemistry Data
setwd("data")
AHH<-read.xlsx("AHH_Titanium.xlsx", sheetIndex=1)
setwd(AHH_Ti_Dir)
View(AHH)
test <- AHH
AHH <- AHH
AHH <- AHH[AHH$FIN_TI!=0,]
AHH <- AHH[AHH$DG_TI!=150,]
AHH <- AHH
hist(AHH$TAP_TI)
hist(AHH$DG_TI)
hist(AHH$Q20TI-AHH$Q11TI)
plot(AHH$TAP_TI,AHH$Q20TI-AHH$Q10TI)
plot(AHH$TAP_TI,AHH$Q20TI-AHH$Q11TI)
cor(AHH$TAP_TI,AHH$Q20TI-AHH$Q10TI)
cor(AHH$TAP_TI,AHH$Q20TI-AHH$Q11TI)
plot(AHH$TAP_TI+AHH$DG_TI,AHH$FIN_TI-AHH$Q11TI)
cor(AHH$TAP_TI+AHH$DG_TI,AHH$FIN_TI-AHH$Q11TI)
lm.fit <- lm(FIN_TI~I(TAP_TI+DG_TI),data=AHH)
summary(lm.fit)
lm.full <- lm(FIN_TI~.-NO_HT_LONG-Q20TI,data=AHH)
summary(lm.full)
AHHtotalAdditions <- AHH$TAP_TI+AHH$DG_TI
AHHtotalTiChange <- AHH$FIN_TI-AHH$Q10TI
plot(AHHtotalAdditions,AHHtotalTiChange,
main = "AHH Ti Change \nQ10 to Final",
xlab=paste("Additions \nCorrelation = ",round(cor(AHHtotalAdditions,AHHtotalTiChange),4)
),
ylab="Ti Change"
)
cor(totalTiChange,totalAdditions)
AHHrecovery <- mean(AHHtotalTiChange)/mean(AHHtotalAdditions)
AHHrecovery
HH$TYPE <- rep("HH",nrow(HH))
View(HH)
AHH$TYPE <- rep("AHH",nrow(HH))
AHH$TYPE <- rep("AHH",nrow(AHH))
test <- rbind(AHH,HH)
plot(test$TAP_TI+test$DG_TI,test$FIN_TI-test$Q10TI)
plot(test$TAP_TI+test$DG_TI,test$FIN_TI-test$Q10TI,col=test$TYPE)
plot(test$TAP_TI+test$DG_TI,test$FIN_TI-test$Q10TI,col=factor(test$TYPE))
qplot(test$TAP_TI+test$DG_TI,test$FIN_TI-test$Q10TI,col=factor(test$TYPE))
qplot(test$TAP_TI+test$DG_TI,test$FIN_TI-test$Q10TI,pch=factor(test$TYPE))
qplot(test$TAP_TI+test$DG_TI,test$FIN_TI-test$Q10TI,pch=factor(test$TYPE),col=factor(test$TYPE))
HH <- HH[(HH$TAP_TI+HH$DG_TI)>100,]
test <- rbind(AHH,HH)
qplot(test$TAP_TI+test$DG_TI,test$FIN_TI-test$Q10TI,pch=factor(test$TYPE),col=factor(test$TYPE))
qplot(test$Q10TI)
summary(test$Q10TI)
qplot(test$FIN_TI)
q11 <- hist(test$Q11TI)                     # centered at 4
p2 <- hist(test$FIN_TI)                     # centered at 6
plot( q11, col=rgb(0,0,1,1/4), xlim=c(0,max(test$FIN_TI)))  # first histogram
plot( p2, col=rgb(1,0,0,1/4), xlim=c(0,10), add=T)  # second
q11 <- hist(HH$Q11TI)                     # centered at 4
p2 <- hist(HH$FIN_TI)                     # centered at 6
plot( q11, col=rgb(0,0,1,1/4), xlim=c(0,max(test$FIN_TI)))  # first histogram
plot( p2, col=rgb(1,0,0,1/4), xlim=c(0,10), add=T)  # second
q11 <- hist(HH$Q11TI)                     # centered at 4
p2 <- hist(HH$FIN_TI)                     # centered at 6
plot( q11, col=rgb(0,0,1,1/4), xlim=c(0,max(test$FIN_TI)))  # first histogram
plot( p2, col=rgb(1,0,0,1/4), xlim=c(0,10), add=T)  # second
q11 <- hist(AHH$Q11TI)                     # centered at 4
p2 <- hist(AHH$FIN_TI)                     # centered at 6
plot( q11, col=rgb(0,0,1,1/4), xlim=c(0,max(test$FIN_TI)))  # first histogram
plot( p2, col=rgb(1,0,0,1/4), xlim=c(0,10), add=T)  # second
q11 <- hist(AHH$Q10TI)                     # centered at 4
p2 <- hist(AHH$FIN_TI)                     # centered at 6
plot( q11, col=rgb(0,0,1,1/4), xlim=c(0,max(test$FIN_TI)))  # first histogram
plot( p2, col=rgb(1,0,0,1/4), xlim=c(0,10), add=T)  # second
q11 <- hist(HH$Q10TI)                     # centered at 4
p2 <- hist(HH$FIN_TI)                     # centered at 6
plot( q11, col=rgb(0,0,1,1/4), xlim=c(0,max(test$FIN_TI)))  # first histogram
plot( p2, col=rgb(1,0,0,1/4), xlim=c(0,10), add=T)  # second
LoadLibraries <- function(){
library(ISLR)
library(ggplot2)
library(caret)
library(AppliedPredictiveModeling)
library(kernlab)
}
LoadLibraries()
data(spam)
set.seed(333)
smallSpam <- spam[sample(nrow(spam),size=10),]
spamLabel <- (smallSpam$type=="spam")+1
plot(smallSpam$capitalAve,col=spamLabel)
data(concrete)
set.seed(1000)
inTrain <- createDataPartition(mixtures$CompressiveStrength,p=3/4)[[1]]
training <- mixtures[inTrain,]
testing <- mixtures[-inTrain,]
summary(concrete$Superplasticizer)
set.seed(3433)
data(AlzheimerDisease)
adData <- data.frame(diagnosis,predictors)
head(adData)
View(adData)
inTrain <- createDataPartition(adData$diagnosis,p=3/4)[[1]]
training <- adData[inTrain,]
testing <- adData[-inTrain,]
preProcess(adData[inTrain,grep("^IL",names(adData))],
method='pca',
thresh=0.9)
####Project 1 Directory Setup####
CourseDir<- getwd()
if (CourseDir!="~/DataScience/PracticalMachineLearning"){
print("Changing Directory")
setwd("~/DataScience/PracticalMachineLearning")
CourseDir<- getwd()
}
####Create Directory for data and figures####
if (!file.exists("Project1")) {
print("Creating Project1 Directory")
dir.create("Project1")
dir.create("Project1/data")
dir.create("Project1/figures")
}
ProjDir <- getwd()
if (ProjDir!="~/DataScience/PracticalMachineLearning/Project1"){
print("Changing Directory")
setwd(paste(CourseDir,"/Project1",sep=""))
}
ProjDir <- getwd()
####Download file####
if(!"pml-training.csv" %in% dir("data")){
print("Downloading Activity Data")
setwd(paste(ProjDir,"/data",sep=""))
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
destfile = "pml-training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
destfile = "pml-testing.csv")
setwd(ProjDir)
}
####Importing Data####
if(!"pml.training" %in% ls()){
print("Importing Activity Data into Environment")
setwd(paste(ProjDir,"/data",sep=""))
pml.training <- read.csv("pml-training.csv")
setwd(ProjDir)
}
####Cleaning Data####
library(plyr)
totalNum <- 0
for(i in 1:ncol(pml.training)){
print(is.numeric(pml.training[,i]))
totalNum = totalNum+is.numeric(pml.training[,i])
}
totalNum/ncol(pml.training)
nsv <- nearZeroVar(pml.training,saveMetrics=TRUE)
nsv
class(nsv)
nsv$nzv
pml.train <- pml.training[,nsv$nzv]
dim(pml.train)
nearZeroVar(pml.train,saveMetrics=TRUE)
nsv <- nearZeroVar(pml.training,saveMetrics=TRUE)
pml.train <- pml.training[,!nsv$nzv]
dim(pml.train)
nearZeroVar(pml.train,saveMetrics=TRUE)
dim(pml.train)
View(pml.train)
View(pml.training)
preProc <- preProcess(pml.train[,-1:6], method="pca",pcaComp=3)
preProc <- preProcess(pml.train[,-(1:6)], method="pca",pcaComp=3)
preProc <- preProcess(pml.train[,-(1:6)], method="pca",pcaComp=2)
preProc <- preProcess(pml.train[,-c(1:6)], method="pca",pcaComp=2)
totalNum <- 0
for(i in 1:ncol(pml.train)){
print(is.numeric(pml.train[,i]))
totalNum = totalNum+is.numeric(pml.training[,i])
}
preProc <- preProcess(pml.train[,-(1:6,ncol(pml.train))], method="pca",pcaComp=2)
preProc <- preProcess(pml.train[,-c(1:6,ncol(pml.train))], method="pca",pcaComp=2)
classy
classy <- predict(preProc,pml.test)
classy <- predict(preProc,pml.testing)
if(!"pml.testing" %in% ls()){
print("Importing Testing Data into Environment")
setwd(paste(ProjDir,"/data",sep=""))
pml.training <- read.csv("pml-testing.csv")
setwd(ProjDir)
}
if(!"pml.training" %in% ls()){
print("Importing Training Data into Environment")
setwd(paste(ProjDir,"/data",sep=""))
pml.training <- read.csv("pml-training.csv")
setwd(ProjDir)
}
setwd(paste(ProjDir,"/data",sep=""))
pml.training <- read.csv("pml-training.csv")
setwd(ProjDir)
pml.testing <- read.csv("pml-testing.csv")
setwd(paste(ProjDir,"/data",sep=""))
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
destfile = "pml-testing.csv")
setwd(ProjDir)
if(!"pml.testing" %in% ls()){
print("Importing Testing Data into Environment")
setwd(paste(ProjDir,"/data",sep=""))
pml.testing <- read.csv("pml-testing.csv")
setwd(ProjDir)
}
classy <- predict(preProc,pml.testing)
classy <- predict(preProc,pml.train)
classy
preProc
classy <- predict(preProc,pml.train[,-c(1:6,ncol(pml.train))])
confusionMatrix(pml.train$classe,classy)
pml.test <- pml.testing[,!nsv$nzv]
testPC <- predict(preProc,pml.test[,-c(1:6,ncol(pml.train))])
modelFit <- train(pml.training$classe ~ .,method="glm",data=classy)
confusionMatrix(pml.test$classe,predict(modelFit,testPC))
modelFit <- train(pml.train$classe ~ .,method="glm",data=classy)
modelFit <- train(pml.train$classe ~ .,method="glm",preProcess="pca",data=classy)
warnings()
setwd("~/DataScience/PracticalMachineLearning")
setwd("~/")
setwd("C:/Users/shegarty/Desktop/Tensiles")
#### Import libraries ####
library(dplyr)
library(ggplot2)
useless <- c(2,4,6,7,10,11,17,19,20,21,23,25,27,31,34,35,36,38,42,44,45,47,49,50,55,56,57,58)
cols <- 1:58
column <-c()
for (i in cols){
if(!i %in% useless){column[i]<-i}
}
column <- na.omit(column)
column <- as.integer(column)
#Tensile<-read.xlsx("Book1.xlsx", sheetIndex=1,colIndex=column)
Tensile<- read.csv("Book1.csv")
Tensile$StartDate <- as.Date(Tensile$StartDate,format="%m/%d/%Y")
Tensile$EndDate <- as.Date(Tensile$EndDate,format="%m/%d/%Y")
Tens <- Tensile[,column]
summary(Tens)
Tens <- Tens[Tens$Total.Elongation>0&Tens$Total.Elongation<50,]
summary(Tens)
Tens <- Tens[!is.na(Tens$Heat.Number),]
Tens$Heat.Number <- as.character(Tens$Heat.Number)
Tens <- Tens[grep(pattern="^2[0-9]7[A-Z][0-9][0-9][0-9]$",x=Tens$Heat.Number),]
summary(Tens)
Tens <- Tens[Tens$Load.at.Break>0,]
summary(Tens)
Three57s <- Tens[Tens$Procedure.Name=="357",]
Tens <- Tens[Tens$Diameter>0.4,]
summary(Tens)
qplot(Pretest.Punch.Length,Total.Elongation,data=Tens,col=Procedure.Name)
levels(Tens$Procedure.Name)
summary(Tens$Procedure.Name)
Tens <- Tens[!is.na(Tens$Procedure.Name),]
Tens <- Tens[Tens$Procedure.Name!="357",]
Tens <- Tens[Tens$Tested.By!="Default",]
qplot(Pretest.Punch.Length,Total.Elongation,data=Tens[Tens$Procedure.Name=="HH"&
Tens$Pretest.Punch.Length>1.96&
Tens$Pretest.Punch.Length<2.0,])
qplot(Stress.at.Offset,Total.Elongation,data=Tens[Tens$Procedure.Name=="HH"&
Tens$Pretest.Punch.Length>1.96&
Tens$Pretest.Punch.Length<2.0&
Tens$Stress.at.Offset>100000,])
nrow(Tens[Tens$Procedure.Name=="HH"&
Tens$Pretest.Punch.Length>1.96&
Tens$Pretest.Punch.Length<2.0,])
#HH heats
HH <- Tens[Tens$Procedure.Name=="HH",]
quantile(HH$Total.Elongation,c(0.05,0.95))
test <- HH[!is.na(HH$StartDate),]
hist(test$Total.Elongation)
quantile(test$Total.Elongation,c(0.05,0.95))
summary(test$StartDate)
length(test$Total.Elongation[test$StartDate>as.Date("2015-01-01")])
quantile(test$Total.Elongation[test$StartDate>as.Date("2015-01-01")])
quantile(test$Total.Elongation[test$StartDate>as.Date("2015-01-01")],
c(0.05,0.95))
hist(test$Total.Elongation[test$StartDate>as.Date("2015-01-01")])
t.test(test$Total.Elongation[test$StartDate>as.Date("2015-01-01")])
# What does the distribution of AHH Elongations look like for each Operating Tech?
qplot(Total.Elongation,
data=HH[HH$StartDate > as.Date("2015-01-01")&grepl("AHH",HH$SECTION),],
fill=factor(Tested.By), geom="bar")
# What are the qualities of AHH tests that fall below 9% elongation?
HH[HH$StartDate > as.Date("2015-01-01")&grepl("AHH",HH$SECTION)&HH$Total.Elongation<9,]
# How does variation of stress at Offset and total elongation compare on a similar scale?
quantile(scale(HH$Stress.at.Offset),probs=c(0.05,0.95))
quantile(scale(HH$Total.Elongation),probs=c(0.05,0.95))
# Calculates Average Elongation as a function of Operating Tech
HH %>% filter(StartDate > as.Date("2015-01-01"),grepl("AHH",HH$SECTION)) %>%
group_by(Tested.By) %>%
summarize(Elong = mean(Total.Elongation),
SD.Elong.frac = sd(Total.Elongation)/mean(Total.Elongation),
SD.Stress.frac = sd(Stress.at.Offset)/mean(Stress.at.Offset)
) %>%
arrange(desc(Elong))
# How much can Stress at Offset vary for the average Elongation?
Tens %>% filter(Stress.at.Offset==median(Stress.at.Offset),
Tensile.Strength==median(Tensile.Strength)) %>%
summarize(Varied=sd(Total.Elongation))
HH %>% filter(StartDate > as.Date("2015-01-01"),grepl("AHH",HH$SECTION)) %>%
group_by(Tested.By) %>%
summarize(Elong = mean(Total.Elongation),
SD.Elong.frac = sd(Total.Elongation)/mean(Total.Elongation),
SD.Yield.frac = sd(Stress.at.Offset)/mean(Stress.at.Offset),
SD.Tensile.frac = sd(Tensile.Strength)/mean(Tensile.Strength)
) %>%
arrange(desc(Elong))
Tens %>% filter(Stress.at.Offset==median(Stress.at.Offset),
Tensile.Strength==median(Tensile.Strength)) %>%
summarize(Varied=sd(Total.Elongation))
####Project 1 Directory Setup####
CourseDir<- getwd()
if (CourseDir!="~/DataScience/PracticalMachineLearning"){
print("Changing Directory")
setwd("~/DataScience/PracticalMachineLearning")
CourseDir<- getwd()
}
####Create Directory for data and figures####
if (!file.exists("Project1")) {
print("Creating Project1 Directory")
dir.create("Project1")
dir.create("Project1/data")
dir.create("Project1/figures")
}
ProjDir <- getwd()
if (ProjDir!="~/DataScience/PracticalMachineLearning/Project1"){
print("Changing Directory")
setwd(paste(CourseDir,"/Project1",sep=""))
}
ProjDir <- getwd()
####Download file####
if(!"pml-training.csv" %in% dir("data")){
print("Downloading Activity Data")
setwd(paste(ProjDir,"/data",sep=""))
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
destfile = "pml-training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
destfile = "pml-testing.csv")
setwd(ProjDir)
}
####Importing Data####
if(!"pml.training" %in% ls()){
print("Importing Training Data into Environment")
setwd(paste(ProjDir,"/data",sep=""))
pml.training <- read.csv("pml-training.csv")
setwd(ProjDir)
}
if(!"pml.testing" %in% ls()){
print("Importing Testing Data into Environment")
setwd(paste(ProjDir,"/data",sep=""))
pml.testing <- read.csv("pml-testing.csv")
setwd(ProjDir)
}
####Cleaning Data####
library(caret)
#### PreProcessing: Setting aside classe data ####
classe <- pml.training$classe
#### PreProcessing: Identify Near Zero Variability Predictors####
nsv <- nearZeroVar(pml.training,saveMetrics=TRUE)
pml.train <- pml.training[,!nsv$nzv]
dim(pml.train)
#### PreProcessing: Remove Precalculated Statistics columns ####
pml.train <- pml.train[,colSums(is.na(pml.train))<19216]
dim(pml.train)
#### PreProcessing: Remove Perceived Unecessary Predictors ####
pml.train <- pml.train[,sapply(pml.train,is.numeric)]
if(ncol(pml.train)>33){
pml.train <- pml.train[,-grep("^X|stamp|window",names(pml.train))]
}
dim(pml.train)
#### PreProcessing: Identify Correlated Predictors####
desCor <- cor(pml.train)
summary(desCor[upper.tri(desCor)])
hiCor <- sum(abs(desCor[upper.tri(desCor)])>0.8)
hiCorDes <- findCorrelation(desCor,cutoff=0.75)
pml.train <- pml.train[, -hiCorDes]
dim(pml.train)
descrCor2 <- cor(pml.train[,7:(ncol(pml.train)-1)])
summary(descrCor2[upper.tri(descrCor2)])
#### PreProcessing: Making an equivalent test set####
name <- names(pml.train)
pml.train$classe <- classe
problem_id <- pml.testing$problem_id
pml.test <- pml.testing[,name]
pml.test$problem_id <- problem_id
#### Data Splitting: Create a training set and test set ####
inTrain <- createDataPartition(pml.train$classe,p= 0.25, list= FALSE)
training <- pml.train[inTrain,]
testing <- pml.train[-inTrain,]
#### PreProcessing: Create 5-fold CV####
fitControl <- trainControl(method="repeatedcv",
number=5)
#### PreProcessing: Principle Component Analysis####
modelFit <- train(training$classe ~ .,
method="rf",
trControl=fitControl,
data=training)
confusionMatrix(testing$classe,predict(modelFit,testing))
pml.testPC <- predict(preProc,pml.test[,-c(2,ncol(pml.test))])
predict(modelFit,pml.testing)
postResample(predict(modelFit,testing),testing$classe)
####Answers####
answers = predict(modelFit,pml.testing)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
saveRDS(modelFit, "my_model_file_v01.Rds")
answers
